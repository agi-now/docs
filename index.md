---
title: Home
layout: home
nav_order: "1"
---

## The Path to AGI

### 1 Uncontrollable AI

The AI landscape has been notably transformed by the emergence of Large Language Models (LLMs). Their rise has undeniably brought us closer to the horizon of Artificial General Intelligence (AGI). Yet, as with any innovative frontier, there are challenges that LLMs may not overcome.

#### 1.1. Controlling Probablity

One of the nuances of LLMs lies in their control mechanics. While minor calibrations can be made to influence their outcomes, achieving a comprehensive grasp over their intricate functionalities is challenging. For instance, integrating a component as fundamental as long-term memory requires more than just a superficial adjustment. Existing methodologies like the RAG do offer potential solutions, but they are inherently limited and don't genuinely emulate organic memory dynamics. Altering the model's architecture is an option, but it mandates a comprehensive retraining process. And before we even delve into the complexities of alignment, it's evident that mastering the technical aspects alone presents substantial hurdles.

#### 1.2. Trainig Cost

Starting the training process of an LLM from the ground up is a resource-intensive endeavor. This doesn't just impact the exploration of newer architectural innovations but also presents a barrier to entry for independent and smaller-scale researchers. Hence, while LLMs present a tantalizing direction toward AGI, they also raise questions about access, equity, and diversity in research.

Yet, it's pertinent to acknowledge that the current trajectory of AI research heavily leans on the scaling potential of LLMs, fostering optimism about bridging the existing gaps.

### 2 Controllable AI

The notion of Controllable AI extends beyond just creating a compliant machine. It's about understanding and delineating the balance between intelligence and controllability in artificial entities.

#### 2.1. Dealing With Ambiguity

One of the historic challenges algorithmic models grappled with was ambiguity. The idea is to craft an algorithm capable of processing ambiguity with the same depth and nuance as human cognition. By weaving in elements of context and real-world knowledge, the AI could potentially offer a richer, more comprehensive understanding of its environment.

#### 2.2. Conceptual Processing and Learning

Our framework is based on the new pattern-matching techniques that can handle ambiguity. We process raw data (like text, audio, images) into a conceptual representation. It can then trigger different actions that are associated with the concept an agent was able to find in the raw data. For example a question might trigger an action of thinking about an answer and then telling it. But the agent's learning journey doesn't end there. By employing pattern matching on its subsequent actions and the feedback it garners from the environment, the agent is able to understand its influence on its surroundings, continiously building it's world model. This is made possible as both actions and outcomes are represented as graphs, allowing for a deeper, graph-based pattern recognition.

While this framework is ambitious and teeming with complexities, it underscores the potential pathways we could explore as we inch closer to AGI.

---

In sum, as we navigate the intricate maze toward AGI, it becomes crucial to introspect, innovate, and iterate. The strengths and challenges of LLMs, combined with the foundational ideas behind Controllable AI, paint a fascinating canvas for the future of AI research.
